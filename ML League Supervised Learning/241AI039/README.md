## Author
- **Name**: Shriya Bharadwaj  
- **Roll No**: 241AI039 
- **Kaggle Username**: shriyabharadwaj

## Overview

This repository contains my submission for the ML League Supervised Learning competition. This includes exploratory data analysis, multiple regression models, optimization techniques, and final ensembling to minimize Mean Absolute Error (MAE).

---

##  Exploratory Data Analysis (EDA)

- Analyzed feature distributions using histograms
- Identified and dropped unnecessary or redundant columns
- Checked for null values 
- Explored feature correlations and relationships with the target

---

##  Models Tried

### Linear Models

- **Linear Regression**(with GridSearchCV)
- **Ridge Regression** 
- **Lasso Regression** 
- **ElasticNet**

While these models were simple and interpretable, they gave relatively higher MAE compared to tree-based models.

### Tree-based Models (optimized with Optuna+ manually tuned + used in ensemble)

- **Random Forest** 
- **XGBoost** 
- **LightGBM** 

These models significantly outperformed linear models in terms of MAE.

### Others

- **K-Nearest Neighbors (KNN)** and **Support Vector Regression (SVR)** were tested but resulted in high MAE, and were not used in the final ensemble.

---

##  Final Model and Ensembling

Used a combination (ensemble) of the following models:

- **XGBoost**
- **LightGBM**
- **Random Forest**

The final predictions were generated by averaging the outputs of these three models, which achieved the lowest MAE on the validation set.
